# Jeeves Architecture Documentation

## Overview

Jeeves is a semantic memory system for Open-WebUI that captures conversation context, embeds it with `sentence-transformers`, and stores it in **Qdrant** vector database. The system consists of three microservices:

1. **Memory API** â€” Conversation memory storage and retrieval (port 8000)
2. **Extractor API** â€” Media-to-text extraction (images, audio, PDF, code) (port 8002)
3. **Pragmatics API** â€” Intent classification (save vs. other) (port 8002)
4. **Qdrant** â€” Vector database for semantic search (port 6333)

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Open-WebUI (User)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                              â”‚
         â”‚ (Conversation with attachment)              â”‚
         â–¼                                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Filter.inlet()     â”‚              â”‚  Assistant Response â”‚
    â”‚  (memory.filter.py) â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  (User reads LLM)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                  â–²
         1. Extract files/images                 â”‚
         2. Save chunks to memory                â”‚ 4. Inject context
         3. Search for context                   â”‚    if found
         4. Inject into request                  â”‚
               â”‚                                  â”‚
               â–¼                                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          Memory API (FastAPI)                       â”‚
    â”‚  Routes: /api/memory/save, /search, /summaries      â”‚
    â”‚                                                     â”‚
    â”‚  1. Receive chunks from filter                     â”‚
    â”‚  2. Classify importance (Pragmatics intent check)  â”‚
    â”‚  3. Embed with SentenceTransformer                 â”‚
    â”‚  4. Upsert to Qdrant                               â”‚
    â”‚  5. Search for existing context                    â”‚
    â”‚  6. Return context + status to filter              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                    â”‚
        Embedding â”‚                    â”‚ Search/Upsert
                 â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”
    â”‚  Qdrant Vector Database              â”‚
    â”‚  Collection: user_memory_collection   â”‚
    â”‚  Dimension: 768 (all-mpnet-base-v2)  â”‚
    â”‚  Similarity: COSINE                  â”‚
    â”‚  Threshold: 0.35 (improved recall)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
         â”‚                    â”‚ Chunks from files/images
         â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Pragmatics â”‚       â”‚  Extractor API            â”‚
    â”‚ Classifier â”‚       â”‚  â€¢ Image â†’ LLaVA descr.   â”‚
    â”‚            â”‚       â”‚  â€¢ Audio â†’ Whisper        â”‚
    â”‚ Save vs    â”‚       â”‚  â€¢ PDF â†’ PyMuPDF text     â”‚
    â”‚ Other      â”‚       â”‚  â€¢ Code â†’ Chunk by func   â”‚
    â”‚ Intent     â”‚       â”‚                           â”‚
    â”‚ (0.70 thr) â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow

### 1. User sends message with attachment

```
User â†’ Open-WebUI â†’ Filter.inlet() {

  Step 1: Extract
  â”œâ”€ Image URLs (data:, http, local) â†’ Extractor API
  â”œâ”€ File uploads â†’ Extractor API (PDF, text, code, audio)
  â””â”€ Result: List of text chunks with source metadata

  Step 2: Save Chunks
  â”œâ”€ POST /api/memory/save for each chunk
  â”œâ”€ Memory API embeds and stores
  â””â”€ Status: saved | saved_with_context | skipped

  Step 3: Search Memory
  â”œâ”€ POST /api/memory/save (with user message)
  â”œâ”€ Memory API searches Qdrant for similar past memories
  â””â”€ Returns: existing_context (list of past messages)

  Step 4: Inject Context
  â”œâ”€ If context found, prepend to user message
  â”‚  Format: "### Previous conversation context ###\n- ..."
  â””â”€ Modified message sent to LLM

} â†’ LLM â†’ Response â†’ User reads on screen
```

### 2. Memory save flow

```
Filter submits chunk â†’ Memory API {

  1. Parse request (user_id, messages, source_type)

  2. Check importance (unless skip_classifier=True for documents)
     â”œâ”€ Extract text from content
     â”œâ”€ Use KeyBERT + heuristics to detect keywords
     â”œâ”€ If casual/musing â†’ skip (e.g., "just curious")
     â””â”€ If important â†’ proceed

  3. Embed with SentenceTransformer
     â”œâ”€ Model: all-mpnet-base-v2
     â”œâ”€ Output: 768-dim vector, L2-normalized
     â””â”€ Time: ~50ms per message

  4. Generate summary (optional)
     â”œâ”€ Model: distilbart-cnn-12-6 (summarizer)
     â”œâ”€ Device: CPU or GPU (configurable)
     â””â”€ Stored alongside full text

  5. Upsert to Qdrant
     â”œâ”€ ID: deterministic UUID(user_id, content_hash)
     â”œâ”€ Vector: 768-dim embedding
     â”œâ”€ Metadata: {user_id, chunk_index, section_title, source, timestamp}
     â””â”€ Payload: {summary, user_text, full_content}

  6. Search for existing context (optional)
     â”œâ”€ Query same embedding against Qdrant
     â”œâ”€ Threshold: 0.35 (COSINE similarity)
     â”œâ”€ Top-k: 5 results
     â””â”€ Filter by user_id

  7. Return status + context to filter
     â”œâ”€ Status: saved | saved_with_context | context_found | skipped
     â””â”€ Existing context items for injection

} â†’ Response to filter â†’ Filter injects into request
```

### 3. Context injection

```
Existing Context Items (from Qdrant search):

[
  {
    "user_text": "I need to set up CI/CD for our Python project",
    "summary": "User asked about CI/CD setup for Python...",
    "source_type": "prompt",
    "source_name": "~30 words from user prompt"
  },
  {
    "user_text": "[Image]: The dashboard shows...",
    "source_type": "image",
    "source_name": "uploaded_image_0"
  }
]

Formatted for injection:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ### Previous conversation context ###  â”‚
â”‚ - I need to set up CI/CD...            â”‚
â”‚ - [Image]: The dashboard shows...      â”‚
â”‚ ### End of context ###                 â”‚
â”‚                                        â”‚
â”‚ [User's current message]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User message â†’ LLM (now has full context)
```

---

## Component Details

### Memory API (`tools-api/memory/`)

**Files:**

- `main.py` â€” FastAPI app setup, middleware, health check
- `api/memory.py` â€” Router with `/save` and `/search` endpoints
- `services/embedder.py` â€” SentenceTransformer embedding
- `services/qdrant_client.py` â€” Lazy singleton Qdrant connection
- `services/summarizer.py` â€” Optional DistilBART summarization
- `services/fact_extractor.py` â€” KeyBERT importance filtering
- `memory.filter.py` â€” Open-WebUI filter plugin (669 lines, 8 sections)
- `utils/schemas.py` â€” Pydantic models for request/response

**Endpoints:**

```
POST /api/memory/save
  Input: SaveRequest (user_id, messages, model, metadata, source_type)
  Process:
    1. Check importance (KeyBERT)
    2. Embed with SentenceTransformer
    3. Search Qdrant for similar context
    4. Upsert to Qdrant
    5. Optionally summarize
  Output: {status, existing_context, summary}

POST /api/memory/search
  Input: SearchRequest (user_id, query_text, top_k)
  Process:
    1. Embed query with SentenceTransformer
    2. Search Qdrant with score_threshold
    3. Filter by user_id
  Output: {results: [{user_text, summary, score, source}]}

POST /api/memory/summaries
  Input: SearchRequest
  Output: {summaries: [{summary, score}]}

GET /health
  Output: {status, memory_api: ok, qdrant: ok}

GET /api/memory/filter
  Output: Source code of memory.filter.py (for Open-WebUI parsing)
```

**Configuration (Environment):**

```
QDRANT_HOST=qdrant                    # Qdrant service hostname
QDRANT_PORT=6333                      # Qdrant HTTP port
INDEX_NAME=user_memory_collection     # Collection name
EMBEDDING_PROVIDER=sentence_transformers
SUMMARY_MODEL=sshleifer/distilbart-cnn-12-6
SUMMARY_DEVICE=cpu                    # cpu or 0,1,2,... for GPU
HF_HOME=/models                       # HuggingFace cache directory
STORE_VERBATIM=true|false             # Store full text alongside summaries
```

---

### Extractor API (`tools-api/extractor/`)

**Purpose:** Extract text from media (images, audio, PDF, code)

**Files:**

- `main.py` â€” FastAPI app, startup model preloading
- `api/extractor.py` â€” `/extract` endpoint, content-type routing
- `services/image_extractor.py` â€” LLaVA or Florence image description
- `services/audio_extractor.py` â€” Whisper transcription
- `services/pdf_extractor.py` â€” PyMuPDF text extraction
- `services/chunker.py` â€” Text segmentation by function/class/heading

**Models:**

```
Image Extraction:
  â€¢ LLaVA-1.5-7B (4-bit, ~4GB VRAM) â€” Default, lazy-loaded at startup
  â€¢ Florence-2 (fallback if LLaVA unavailable)
  â€¢ Query: "Describe this image in detail: [image]"

Audio Transcription:
  â€¢ Whisper (base model)
  â€¢ Auto-detects language, returns full transcript

PDF Extraction:
  â€¢ PyMuPDF (fitz)
  â€¢ OCR for scanned documents (if available)

Code Chunking:
  â€¢ Language-aware segmentation (Python, JavaScript, etc.)
  â€¢ Chunks by function/class/import statements
  â€¢ Preserves context (parent class, module)
```

**Endpoint:**

```
POST /api/extract
  Input: {
    content: "base64 or text",
    content_type: "image/png|application/pdf|audio/mp3|text/plain",
    source_name: "filename",
    chunk_size: 500,
    chunk_overlap: 50,
    prompt: "optional guided prompt for images"
  }
  Output: {
    chunks: [{
      content: "extracted text",
      chunk_index: 0,
      chunk_type: "text|heading|function",
      section_title: "optional"
    }]
  }
```

**Configuration:**

```
IMAGE_MODEL=llava-4bit|llava|florence
HF_HOME=/models
DEVICE=cuda|cpu
```

---

### Pragmatics API (`tools-api/pragmatics/`)

**Purpose:** Binary intent classification (save vs. other)

**Model:** DistilBERT fine-tuned for "save memory" intent

- Training data: ~1300 examples (save/recall/other)
- Threshold: 0.70 (conservative: prefers NOT saving when uncertain)
- Latency: ~5ms per sample

**Endpoint:**

```
POST /classify
  Input: {user_id, messages, user_prompt}
  Output: {
    intent: "save",
    confidence: 0.92,
    keywords: ["ci/cd", "setup", "python"]
  }
```

---

### Qdrant Vector Database

**Configuration:**

```
Container: qdrant/qdrant:latest
Ports:
  - 6333 (HTTP API)
  - 5100 (Web Dashboard UI)

Collection: user_memory_collection
  Vector Size: 768 (all-mpnet-base-v2)
  Distance: COSINE
  Storage: /qdrant/storage (mounted to C:/docker-data/qdrant/storage)

Search Parameters:
  - score_threshold: 0.35 (lowered from 0.45 for better recall)
  - top_k: 5 (per search)
  - filter: {key: 'user_id', match: {value: req.user_id}}
```

---

## Docker Architecture

### Multi-stage Build Strategy

Each service uses **3-layer caching** to minimize rebuild time:

```dockerfile
# Layer 1: System Dependencies (rarely changes)
FROM python:3.11-slim
RUN apt-get update && apt-get install -y --no-install-recommends \
  [ffmpeg, git, gcc, etc.]

# Layer 2: Python Packages (cached if requirements.txt unchanged)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Layer 3: Application Code (changes frequently)
COPY . /app
CMD ["uvicorn", "main:app", ...]
```

**Cache Performance:**

- **First build:** 35+ minutes (downloads torch ~900MB, other packages)
- **Second build (code change only):** 2 seconds (all layers cached)
- **First build (new requirements):** Full rebuild (~5-10 min with pip)

### Docker Compose

**Memory API** (`tools-api/memory/docker-compose.yaml`):

```yaml
services:
  memory_api:
    build:
      context: ..
      dockerfile: memory/Dockerfile
    environment: QDRANT_HOST=qdrant
      QDRANT_PORT=6333
      INDEX_NAME=user_memory_collection
    ports: [8000:8000]
    volumes: [C:/docker-data/models:/models]
    networks: [webtools_network]
```

**Network:** External network `webtools_network` (shared with Ollama, Open-WebUI)

---

## Integration with Open-WebUI

### Filter Plugin Contract

Open-WebUI calls filter at two points:

**1. inlet (before LLM sees the message)**

```python
async def inlet(body: dict, __event_emitter__, __user__: dict):
    # Extract files/images
    # Search memory
    # Inject context into body["messages"]
    # Emit status events for UI
    return modified_body
```

**2. outlet (after LLM response)**

```python
async def outlet(body: dict, __event_emitter__, __user__: dict):
    # Currently no-op
    # Could save assistant responses or update context
    pass
```

### Status Events

Filter emits UI updates as the process runs:

```
âœ¨ Processing...          (initial)
ğŸ“„ Saving 5 chunks...     (during extraction)
ğŸ§  Found 3 memories       (after search)
  â€¢ "Set up CI/CD..."
  â€¢ ğŸ–¼ uploaded_image_0
  â€¢ [Highlighted previous context]
âœ” Context injected        (done)
```

---

## Performance Metrics

### Latency (per conversation turn)

| Operation      | Time  | Notes                                    |
| -------------- | ----- | ---------------------------------------- |
| Extract images | 2-5s  | ~1s per image (LLaVA inference)          |
| Extract PDF    | 1-3s  | Depends on pages                         |
| Embed message  | 50ms  | SentenceTransformer                      |
| Search Qdrant  | 100ms | Cosine similarity, top-5                 |
| Filter total   | <10s  | (includes extraction, search, injection) |

### Storage

| Aspect                    | Size   | Notes              |
| ------------------------- | ------ | ------------------ |
| Model: all-mpnet-base-v2  | ~430MB | Embeddings         |
| Model: distilbart-cnn     | ~250MB | Summarization      |
| Model: LLaVA-1.5-7B       | ~4GB   | 4-bit quantized    |
| Qdrant: per 1000 memories | ~50MB  | Vectors + metadata |

---

## Deployment Notes

### Prerequisites

- Docker with GPU support (for LLaVA, optional for memory)
- NVIDIA CUDA 12.1+ (if using GPU for image extraction)
- 16GB+ RAM (8GB minimum, 16GB recommended)
- 10GB+ disk space (for models)

### Environment Variables

**Memory API:**

```bash
export QDRANT_HOST=qdrant
export QDRANT_PORT=6333
export SUMMARY_DEVICE=cpu  # or 0 for GPU:0
export STORE_VERBATIM=true
```

**Extractor API:**

```bash
export IMAGE_MODEL=llava-4bit  # or llava, florence
export DEVICE=cuda  # or cpu
export HF_HOME=/models
```

### Startup Sequence

1. Qdrant starts, initializes storage
2. Memory API starts, connects to Qdrant, loads embedding model
3. Extractor API starts, **preloads LLaVA at startup** (eliminates 8-12s first-request latency)
4. Open-WebUI detects filter via `GET /.well-known/ai-plugin.json`
5. First user message triggers extraction/memory flow

---

## Key Decisions & Trade-offs

| Decision                               | Rationale                                  | Trade-off                                               |
| -------------------------------------- | ------------------------------------------ | ------------------------------------------------------- |
| Cosine similarity (0.35 threshold)     | Better semantic matching, higher recall    | Some false positives (mitigated by relevance filtering) |
| SentenceTransformer 768-dim            | Fast embedding, good quality               | Requires VRAM, ~430MB model                             |
| LLaVA 4-bit quantization               | Fits in ~4GB VRAM                          | Slight quality loss (~5% accuracy)                      |
| Qdrant over Pinecone/Weaviate          | Self-hosted, no API costs, flexible        | Requires management, backups                            |
| DistilBERT for intent (0.70 threshold) | Conservative: avoid saving trivial queries | Misses some important context (acceptable trade-off)    |
| Lazy model loading (except extractor)  | Reduces startup time                       | First image request slower (now preloaded)              |

---

## Future Enhancements

1. **Adaptive Threshold:** Dynamically adjust Qdrant threshold based on collection size and user feedback
2. **Model Selection:** Allow users to choose embedding model (e.g., bge-large-en for better performance)
3. **Multi-user Isolation:** Enhanced privacy controls per workspace
4. **Context Summarization:** Compress old memories to reduce latency for large datasets
5. **Duplicate Detection:** Avoid storing near-duplicate memories
6. **Feedback Loop:** User ratings of injected context to improve ranking

---

## Quick Reference

### Endpoints

| Service    | Method | Route                   | Purpose         |
| ---------- | ------ | ----------------------- | --------------- |
| Memory     | POST   | `/api/memory/save`      | Save + search   |
| Memory     | POST   | `/api/memory/search`    | Search only     |
| Memory     | POST   | `/api/memory/summaries` | Get summaries   |
| Memory     | GET    | `/health`               | Health check    |
| Extractor  | POST   | `/api/extract`          | Extract text    |
| Pragmatics | POST   | `/classify`             | Classify intent |

### Key Files

```
c:\Code\jeeves\
â”œâ”€â”€ tools-api/
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ main.py              â† FastAPI app
â”‚   â”‚   â”œâ”€â”€ api/memory.py        â† /save, /search routes
â”‚   â”‚   â”œâ”€â”€ memory.filter.py     â† Open-WebUI filter (669 lines, 8 sections)
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ embedder.py      â† SentenceTransformer
â”‚   â”‚   â”‚   â”œâ”€â”€ qdrant_client.py â† Qdrant singleton
â”‚   â”‚   â”‚   â””â”€â”€ summarizer.py    â† DistilBART
â”‚   â”‚   â””â”€â”€ Dockerfile           â† 3-layer caching
â”‚   â”œâ”€â”€ extractor/
â”‚   â”‚   â”œâ”€â”€ main.py              â† Startup model preloading
â”‚   â”‚   â”œâ”€â”€ api/extractor.py     â† /extract endpoint
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ image_extractor.py  â† LLaVA/Florence
â”‚   â”‚   â”‚   â””â”€â”€ audio_extractor.py  â† Whisper
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ pragmatics/
â”‚       â”œâ”€â”€ server.py            â† /classify endpoint
â”‚       â”œâ”€â”€ services/
â”‚       â”‚   â””â”€â”€ classifier.py    â† DistilBERT
â”‚       â””â”€â”€ Dockerfile
â””â”€â”€ ARCHITECTURE.md              â† This file
```

---

## Related Documentation

- `.github/copilot-instructions.md` â€” Detailed operational guidelines
- `README.md` â€” Setup and usage instructions
- Individual service Dockerfiles â€” Build details and dependencies
