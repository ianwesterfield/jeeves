networks:
  webtools_network:
    external: true

services:
  # -------------------------------------------------
  # Ollama
  # -------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    runtime: nvidia
    environment:
      - OLLAMA_HOST=0.0.0.0
    ports:
      - "11434:11434"
    volumes:
      - C:\\docker-data\\ollama\\.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: 2g
    restart: unless-stopped
    networks:
      - webtools_network

  # -------------------------------------------------
  # Qdrant
  # -------------------------------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    environment:
      - QDRANT__LOG_LEVEL=DEBUG
      # Flush WAL more frequently to reduce corruption risk on shutdown
      - QDRANT__STORAGE__OPTIMIZERS__FLUSH_INTERVAL_SEC=1
    ports:
      - "5100:5100" # Dashboard UI
      - "6333:6333" # HTTP API
    volumes:
      - C:/docker-data/qdrant/storage:/qdrant/storage
    restart: unless-stopped
    # Give Qdrant time to flush WAL and close cleanly
    stop_grace_period: 30s
    networks:
      - webtools_network

  # -------------------------------------------------
  # Memory API
  # -------------------------------------------------
  memory_api:
    build:
      context: ./tools-api
      dockerfile: memory/Dockerfile
      args:
        BASE_REGISTRY: ${BASE_REGISTRY:-jeeves}
    container_name: memory_api
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - INDEX_NAME=user_memory_collection
      - EMBEDDING_PROVIDER=sentence_transformers
      - SUMMARY_MODEL=sshleifer/distilbart-cnn-12-6
      - SUMMARY_DEVICE=0 # Use cpu for summarizer (or 0 for GPU)
      - HF_HOME=/models
    volumes:
      - C:/docker-data/models:/models
    ports:
      - "8000:8000"
    depends_on:
      - qdrant
    restart: unless-stopped
    networks:
      - webtools_network

  # -------------------------------------------------
  # Pragmatics API
  # -------------------------------------------------
  pragmatics_api:
    build:
      context: ./tools-api/pragmatics
      dockerfile: Dockerfile
      args:
        BASE_REGISTRY: ${BASE_REGISTRY:-jeeves}
    container_name: pragmatics_api
    environment:
      - CLASSIFIER_MODEL=distilbert_memory
    depends_on:
      - memory_api
    restart: unless-stopped
    networks:
      - webtools_network

  # -------------------------------------------------
  # Extractor API (Media -> Text)
  # -------------------------------------------------
  extractor_api:
    build:
      context: ./tools-api/extractor
      dockerfile: Dockerfile
      args:
        BASE_REGISTRY: ${BASE_REGISTRY:-jeeves}
    container_name: extractor_api
    ports:
      - "8002:8002"
    environment:
      - WHISPER_MODEL=base
      - IMAGE_MODEL=llava-4bit # Options: llava-4bit (~4GB), llava (~14GB), florence (~4GB)
      - HF_HOME=/models
    volumes:
      - C:/docker-data/models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - webtools_network

  # -------------------------------------------------
  # Orchestrator API (Agentic Reasoning Engine)
  # -------------------------------------------------
  orchestrator_api:
    build:
      context: ./tools-api/orchestrator
      dockerfile: Dockerfile
    container_name: orchestrator_api
    ports:
      - "8004:8004"
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=llama3.2
      - MEMORY_API_URL=http://memory_api:8000
      - EXECUTOR_API_URL=http://executor_api:8005
      - MAX_PARALLEL_TASKS=5
    depends_on:
      - ollama
      - memory_api
    restart: unless-stopped
    networks:
      - webtools_network

  # -------------------------------------------------
  # Executor API (Polyglot Code Execution)
  # -------------------------------------------------
  executor_api:
    build:
      context: ./tools-api/executor
      dockerfile: Dockerfile
    container_name: executor_api
    ports:
      - "8005:8005"
    environment:
      - DEFAULT_TIMEOUT=30
      - MAX_TIMEOUT=120
    restart: unless-stopped
    networks:
      - webtools_network

  # -------------------------------------------------
  # Open-WebUI
  # -------------------------------------------------
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    depends_on:
      - ollama
    environment:
      - USER_AGENT=WesterfieldCloud-OpenWebUI/1.0
      - GLOBAL_LOG_LEVEL=DEBUG
      - OLLAMA_BASE_URL=http://ollama:11434

      # Note: Set these manually or use docker secrets in production
      # - WEBUI_ADMIN_USERNAME=admin
      # - WEBUI_ADMIN_PASSWORD=changeme

      - RAG_EMBEDDING_ENGINE=ollama
      - EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
      - MEMORY_ENABLED=true
      - MEMORY_SHORT_TERM_MEMORY_DEPTH=15
      - TOOLS_FUNCTION_CALLING_ENABLED=true
      - TOOLS_FUNCTION_CALLING_LLMS=ollama
      - MEMORY_API_URL=http://memory_api:8000
      - TOOLS_DISCOVERY_URL=http://memory_api:8000/api/tools
    ports:
      - "8180:8080" # Host 8180 -> Container 8080 (8080 often reserved by Windows/Hyper-V)
    volumes:
      - c:/docker-data/open-webui/data:/app/backend/data
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 8g
    restart: unless-stopped
    networks:
      - webtools_network
